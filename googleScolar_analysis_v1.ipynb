{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from utils import *\n",
    "from compress_utils import compressed_cpickle, decompressed_cpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(soup, href=True):\n",
    "    \n",
    "    h3_soup = soup.find_all('h3', class_=\"gs_rt\")\n",
    "    gs_rs_soup = soup.find_all('div', class_=\"gs_rs\")\n",
    "    gs_a_group = soup.find_all('div', class_='gs_a')\n",
    "    result = {'topic':[], 'abstract':[], \"year\":[]}\n",
    "    \n",
    "    valid_website = ['openaccess.thecvf.com', 'proceedings.neurips.cc', 'link.springer.com', 'dl.acm.org',\n",
    "         'arxiv.org', 'www.pnas.org', 'www.usenix.org', 'www.sciencedirect.com', 'www.nature.com',\n",
    "          'proceedings.mlr.press']\n",
    "    except_lis = ['ieeexplore.ieee.org']\n",
    "    valid_website = [i for i in valid_website if not i in except_lis]\n",
    "    \n",
    "    text_rule = lambda text:text.replace('.','_')\n",
    "    extract_func_dic = {}\n",
    "    for website in valid_website:\n",
    "        name_ = 'abs_from_'+text_rule(website)\n",
    "        extract_func_dic[website] = eval(name_)\n",
    "    \n",
    "\n",
    "    if href:\n",
    "        result['href'] = []\n",
    "        \n",
    "                \n",
    "    # get topic and abstract and href\n",
    "    for topic_res, abstract_res, attr_res in zip(h3_soup, gs_rs_soup, gs_a_group):\n",
    "        # get hef:\n",
    "        if href:\n",
    "            a = str(topic_res).split(' ')\n",
    "            href_ = [e[6:-1] for e in a if e.__contains__('href')][0]\n",
    "#             print(href_)\n",
    "            if href_:\n",
    "                result['href'].append(href_)\n",
    "            else:\n",
    "                result['href'].append('NaN')\n",
    "                print('else')\n",
    "   \n",
    "                \n",
    "#                 print(topic_res)\n",
    "        \n",
    "        # get year\n",
    "        year = [year for year in range(2015,2022) if attr_res.get_text().__contains__(str(year))]\n",
    "#         print(year[0])\n",
    "        if year:\n",
    "            result['year'].append(year[0])\n",
    "\n",
    "        else:\n",
    "            result['year'].append('NaN')\n",
    "            print('else, not found year, fill with nan')\n",
    "        \n",
    "        \n",
    "        # get topic\n",
    "        topic_text = topic_res.get_text()\n",
    "        b = '[HTML][HTML] '\n",
    "        topic_text = topic_text.replace(b, '')\n",
    "        result['topic'].append(topic_text)\n",
    "        \n",
    "        \n",
    "        # get abstract\n",
    "        website = href_.split('/')[2] \n",
    "        if [i for i in valid_website if i.__contains__(website)]:\n",
    "            abstract = extract_func_dic[website](href_)\n",
    "            result['abstract'].append(abstract)\n",
    "        else:\n",
    "            # google scholar default short abstract\n",
    "            if abstract_res.get_text():\n",
    "                result['abstract'].append(abstract_res.get_text())\n",
    "            else:\n",
    "                result['abstract'].append('NaN')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "# google_url = \"https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5\"\n",
    "# r = requests.get(google_url)\n",
    "# soup = BeautifulSoup(r.text, 'html.parser')\n",
    "# res = get_topic(soup)\n",
    "# # print(res)\n",
    "\n",
    "# for key in res:\n",
    "#     print(key, res[key])\n",
    "    \n",
    "# for contents in zip(res.values()):\n",
    "# #     print(contents)\n",
    "#     for i in range(len(contents)):\n",
    "#         print(list(res.keys())[i], contents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "page\n",
    "- [ok]. 0~30\n",
    "- [ok]. 30~60\n",
    "- [ok]. 60~90\n",
    "- [ok]. 90~120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=0\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=10\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=20\n",
      "else\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=30\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=40\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=50\n",
      "else\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=60\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=70\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=80\n",
      "else\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=90\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=100\n",
      "else\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=110\n",
      "else\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=120\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=130\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=140\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=150\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=160\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=170\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=180\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=190\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=200\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=210\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=220\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=230\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=240\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=250\n",
      "else\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=260\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=270\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=280\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=290\n",
      "else\n"
     ]
    }
   ],
   "source": [
    "google_url = \"https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5\"\n",
    "end = 980\n",
    "dic = {}\n",
    "for i in range(end//10+1)[:30]:\n",
    "    a = google_url + f'&start={i*10}'\n",
    "    print(a)\n",
    "    r = requests.get(a)\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    res = get_topic(soup)\n",
    "#     print(res)\n",
    "#     lis.append()\n",
    "    dic['page_'+str(i)] = res\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['page_0', 'page_1', 'page_2', 'page_3', 'page_4', 'page_5', 'page_6', 'page_7', 'page_8', 'page_9', 'page_10', 'page_11', 'page_12', 'page_13', 'page_14', 'page_15', 'page_16', 'page_17', 'page_18', 'page_19', 'page_20', 'page_21', 'page_22', 'page_23', 'page_24', 'page_25', 'page_26', 'page_27', 'page_28', 'page_29'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': ['Ditto: Fair and robust federated learning through personalization',\n",
       "  'Federated learning for internet of things: Recent advances, taxonomy, and open challenges',\n",
       "  'Federated learning for predicting clinical outcomes in patients with COVID-19',\n",
       "  'A survey on security and privacy of federated learning',\n",
       "  'Oort: Efficient federated learning via guided participant selection',\n",
       "  'A survey on federated learning',\n",
       "  'Communication-efficient federated learning',\n",
       "  'System optimization of federated learning networks with a constrained latency',\n",
       "  'Federated learning based on dynamic regularization',\n",
       "  'Multi-center federated learning'],\n",
       " 'abstract': ['Fairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, that can inherently provide fairness and robustness benefits, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines.',\n",
       "  '… federated learning can offer better privacy preservation than centralized machine learning, \\nit has still privacy concerns. In this paper, first, we present the recent advances of federated \\nlearning towards enabling federated learning-… a taxonomy for federated learning over IoT …',\n",
       "  'Federated learning (FL) is a method used for training artificial intelligence models with data from multiple sources while maintaining data anonymity, thus removing many barriers to data sharing. Here we used data from 20\\u2009institutes across the globe to train a FL model, called EXAM (electronic medical record (EMR) chest X-ray AI model), that predicts the future oxygen requirements of symptomatic patients with COVID-19 using inputs of vital signs, laboratory data and chest X-rays. EXAM achieved an average area under the curve (AUC) >0.92 for predicting outcomes at 24 and 72\\u2009h from the time of initial presentation to the emergency room, and it provided 16% improvement in average AUC measured across all participating sites and an average increase in generalizability of 38% when compared with models trained at a single site using that site’s data. For prediction of mechanical ventilation treatment or death at 24\\u2009h at the largest independent test site, EXAM achieved a sensitivity of 0.950 and specificity of 0.882. In this study, FL facilitated rapid data science collaboration without data exchange and generated a model that generalized across heterogeneous, unharmonized datasets for prediction of clinical outcomes in patients with COVID-19, setting the stage for the broader use of FL in healthcare.',\n",
       "  'AbstractFederated learning (FL) is a new breed of Artificial Intelligence (AI) that builds upon decentralized data and training that brings learning to the edge or directly on-device. FL is a new research area often referred to as a new dawn in AI, is in its infancy, and has not yet gained much trust in the community, mainly because of its (unknown) security and privacy implications. To advance the state of the research in this area and to realize extensive utilization of the FL approach and its mass adoption, its security and privacy concerns must be first identified, evaluated, and documented. FL is preferred in use-cases where security and privacy are the key concerns and having a clear view and understanding of risk factors enable an implementer/adopter of FL to successfully build a secure environment and gives researchers a clear vision on possible research areas. This paper aims to provide a comprehensive study concerning FL’s security and privacy aspects that can help bridge the gap between the current state of federated AI and a future in which mass adoption is possible. We present an illustrative description of approaches and various implementation styles with an examination of the current challenges in FL and establish a detailed review of security and privacy concerns that need to be considered in a thorough and clear context. Findings from our study suggest that overall there are fewer privacy-specific threats associated with FL compared to security threats. The most specific security threats currently are communication bottlenecks, poisoning, and backdoor attacks while inference-based attacks are the most critical to the privacy of FL. We conclude the paper with much needed future research directions to make FL adaptable in realistic scenarios.',\n",
       "  'Abstract:\\xa0Federated Learning (FL) is an emerging direction in distributed machine learning (ML) that enables in-situ model training and testing on edge data. Despite having the same end goals as traditional ML, FL executions differ significantly in scale, spanning thousands to millions of participating devices. As a result, data characteristics and device capabilities vary widely across clients. Yet, existing efforts randomly select FL participants, which leads to poor model and system efficiency.\\nIn this paper, we propose Oort to improve the performance of federated training and testing with guided participant selection. With an aim to improve time-to-accuracy performance in model training, Oort prioritizes the use of those clients who have both data that offers the greatest utility in improving model accuracy and the capability to run training quickly. To enable FL developers to interpret their results in model testing, Oort enforces their requirements on the distribution of participant data while improving the duration of federated testing by cherry-picking clients. Our evaluation shows that, compared to existing participant selection mechanisms, Oort improves time-to-accuracy performance by 1.2X-14.1X and final model accuracy by 1.3%-9.8%, while efficiently enforcing developer-specified model testing criteria at the scale of millions of clients.',\n",
       "  'AbstractFederated learning is a set-up in which multiple clients collaborate to solve machine learning problems, which is under the coordination of a central aggregator. This setting also allows the training data decentralized to ensure the data privacy of each device. Federated learning adheres to two major ideas: local computing and model transmission, which reduces some systematic privacy risks and costs brought by traditional centralized machine learning methods. The original data of the client is stored locally and cannot be exchanged or migrated. With the application of federated learning, each device uses local data for local training, then uploads the model to the server for aggregation, and finally the server sends the model update to the participants to achieve the learning goal. To provide a comprehensive survey and facilitate the potential research of this area, we systematically introduce the existing works of federated learning from five aspects: data partitioning, privacy mechanism, machine learning model, communication architecture and systems heterogeneity. Then, we sort out the current challenges and future research directions of federated learning. Finally, we summarize the characteristics of existing federated learning, and analyze the current practical application of federated learning.',\n",
       "  'AbstractFederated learning (FL) enables edge devices, such as Internet of Things devices (e.g., sensors), servers, and institutions (e.g., hospitals), to collaboratively train a machine learning (ML) model without sharing their private data. FL requires devices to exchange their ML parameters iteratively, and thus the time it requires to jointly learn a reliable model depends not only on the number of training steps but also on the ML parameter transmission time per step. In practice, FL parameter transmissions are often carried out by a multitude of participating devices over resource-limited communication networks, for example, wireless networks with limited bandwidth and power. Therefore, the repeated FL parameter transmission from edge devices induces a notable delay, which can be larger than the ML model training time by orders of magnitude. Hence, communication delay constitutes a major bottleneck in FL. Here, a communication-efficient FL framework is proposed to jointly improve the FL convergence time and the training loss. In this framework, a probabilistic device selection scheme is designed such that the devices that can significantly improve the convergence speed and training loss have higher probabilities of being selected for ML model transmission. To further reduce the FL convergence time, a quantization method is proposed to reduce the volume of the model parameters exchanged among devices, and an efficient wireless resource allocation scheme is developed. Simulation results show that the proposed FL framework can improve the identification accuracy and convergence time by up to 3.6% and 87% compared to standard FL.',\n",
       "  '… Abstract—This paper investigates a wireless federated learning (FL) network with limited \\ncommunication bandwidth, where multiple … and computational capability at the clients, we \\noptimize the federated learning network by maximizing the number of active clients under the …',\n",
       "  'We propose a novel federated learning method for distributively trainingneural network models, where the server orchestrates cooperation between asubset of randomly chosen devices in each round. We view Federated Learningproblem primarily from a communication perspective and allow more device levelcomputations to save transmission costs. We point out a fundamental dilemma, inthat the minima of the local-device level empirical loss are inconsistent withthose of the global empirical loss. Different from recent prior works, thateither attempt inexact minimization or utilize devices for parallelizinggradient computation, we propose a dynamic regularizer for each device at eachround, so that in the limit the global and device solutions are aligned. Wedemonstrate both through empirical results on real and synthetic data as wellas analytical results that our scheme leads to efficient training, in bothconvex and non-convex settings, while being fully agnostic to deviceheterogeneity and robust to large number of devices, partial participation andunbalanced data.',\n",
       "  'Federated learning (FL) can protect data privacy in distributed learningsince it merely collects local gradients from users without access to theirdata. However, FL is fragile in the presence of heterogeneity that is commonlyencountered in practical settings, e.g., non-IID data over different users.Existing FL approaches usually update a single global model to capture theshared knowledge of all users by aggregating their gradients, regardless of thediscrepancy between their data distributions. By comparison, a mixture ofmultiple global models could capture the heterogeneity across various users ifassigning the users to different global models (i.e., centers) in FL. To thisend, we propose a novel multi-center aggregation mechanism . It learns multipleglobal models from data, and simultaneously derives the optimal matchingbetween users and centers. We then formulate it as a bi-level optimizationproblem that can be efficiently solved by a stochastic expectation maximization(EM) algorithm. Experiments on multiple benchmark datasets of FL show that ourmethod outperforms several popular FL competitors. The source code are opensource on Github.'],\n",
       " 'year': [2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021],\n",
       " 'href': ['https://proceedings.mlr.press/v139/li21h.html',\n",
       "  'https://ieeexplore.ieee.org/abstract/document/9460016/',\n",
       "  'https://www.nature.com/articles/s41591-021-01506-3',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0167739X20329848',\n",
       "  'https://www.usenix.org/conference/osdi21/presentation/lai',\n",
       "  'https://www.sciencedirect.com/science/article/pii/S0950705121000381',\n",
       "  'https://www.pnas.org/content/118/17/e2024789118.short',\n",
       "  'https://ieeexplore.ieee.org/abstract/document/9616432/',\n",
       "  'https://arxiv.org/abs/2111.04263',\n",
       "  'https://arxiv.org/abs/2108.08647']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['page_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pathlib\n",
    "\n",
    "path = './Data'\n",
    "pathlib.Path(f'{path}').mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d%m-%H%M\")\n",
    "# print(dt_string)\n",
    "compressed_cpickle(f'./Data/scholar_analysis_FL_{dt_string}', dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Data/all_dic.pbz2', './Data/scholar_analysis_FL_1401-1217.pbz2', './Data/scholar_analysis_FL_1401-1545.pbz2', './Data/scholar_analysis_FL_1401-1633.pbz2', './Data/scholar_analysis_FL_1401-1643.pbz2', './Data/scholar_analysis_FL_1401-1820.pbz2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./Data/scholar_analysis_FL_1401-1820.pbz2'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "pbz_paths = ['/'.join([path,i]) for i in os.listdir(path) if i.__contains__('.pbz2')]\n",
    "print(pbz_paths)\n",
    "newest_file_path = max(pbz_paths, key=os.path.getctime)\n",
    "newest_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Data/scholar_analysis_FL_1401-1820.pbz2'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newest_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['topic', 'abstract', 'year', 'href'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< topic >> Ditto: Fair and robust federated learning through personalization\n",
      "<< abstract >> Fairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, that can inherently provide fairness and robustness benefits, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines.\n",
      "<< year >> 2021\n",
      "<< href >> https://proceedings.mlr.press/v139/li21h.html\n",
      "\n",
      "<< topic >> Federated learning for internet of things: Recent advances, taxonomy, and open challenges\n",
      "<< abstract >> … federated learning can offer better privacy preservation than centralized machine learning, \n",
      "it has still privacy concerns. In this paper, first, we present the recent advances of federated \n",
      "learning towards enabling federated learning-… a taxonomy for federated learning over IoT …\n",
      "<< year >> 2021\n",
      "<< href >> https://ieeexplore.ieee.org/abstract/document/9460016/\n",
      "\n",
      "<< topic >> Federated learning for predicting clinical outcomes in patients with COVID-19\n",
      "<< abstract >> Federated learning (FL) is a method used for training artificial intelligence models with data from multiple sources while maintaining data anonymity, thus removing many barriers to data sharing. Here we used data from 20 institutes across the globe to train a FL model, called EXAM (electronic medical record (EMR) chest X-ray AI model), that predicts the future oxygen requirements of symptomatic patients with COVID-19 using inputs of vital signs, laboratory data and chest X-rays. EXAM achieved an average area under the curve (AUC) >0.92 for predicting outcomes at 24 and 72 h from the time of initial presentation to the emergency room, and it provided 16% improvement in average AUC measured across all participating sites and an average increase in generalizability of 38% when compared with models trained at a single site using that site’s data. For prediction of mechanical ventilation treatment or death at 24 h at the largest independent test site, EXAM achieved a sensitivity of 0.950 and specificity of 0.882. In this study, FL facilitated rapid data science collaboration without data exchange and generated a model that generalized across heterogeneous, unharmonized datasets for prediction of clinical outcomes in patients with COVID-19, setting the stage for the broader use of FL in healthcare.\n",
      "<< year >> 2021\n",
      "<< href >> https://www.nature.com/articles/s41591-021-01506-3\n",
      "\n",
      "<< topic >> A survey on security and privacy of federated learning\n",
      "<< abstract >> AbstractFederated learning (FL) is a new breed of Artificial Intelligence (AI) that builds upon decentralized data and training that brings learning to the edge or directly on-device. FL is a new research area often referred to as a new dawn in AI, is in its infancy, and has not yet gained much trust in the community, mainly because of its (unknown) security and privacy implications. To advance the state of the research in this area and to realize extensive utilization of the FL approach and its mass adoption, its security and privacy concerns must be first identified, evaluated, and documented. FL is preferred in use-cases where security and privacy are the key concerns and having a clear view and understanding of risk factors enable an implementer/adopter of FL to successfully build a secure environment and gives researchers a clear vision on possible research areas. This paper aims to provide a comprehensive study concerning FL’s security and privacy aspects that can help bridge the gap between the current state of federated AI and a future in which mass adoption is possible. We present an illustrative description of approaches and various implementation styles with an examination of the current challenges in FL and establish a detailed review of security and privacy concerns that need to be considered in a thorough and clear context. Findings from our study suggest that overall there are fewer privacy-specific threats associated with FL compared to security threats. The most specific security threats currently are communication bottlenecks, poisoning, and backdoor attacks while inference-based attacks are the most critical to the privacy of FL. We conclude the paper with much needed future research directions to make FL adaptable in realistic scenarios.\n",
      "<< year >> 2021\n",
      "<< href >> https://www.sciencedirect.com/science/article/pii/S0167739X20329848\n",
      "\n",
      "<< topic >> Oort: Efficient federated learning via guided participant selection\n",
      "<< abstract >> Abstract: Federated Learning (FL) is an emerging direction in distributed machine learning (ML) that enables in-situ model training and testing on edge data. Despite having the same end goals as traditional ML, FL executions differ significantly in scale, spanning thousands to millions of participating devices. As a result, data characteristics and device capabilities vary widely across clients. Yet, existing efforts randomly select FL participants, which leads to poor model and system efficiency.\n",
      "In this paper, we propose Oort to improve the performance of federated training and testing with guided participant selection. With an aim to improve time-to-accuracy performance in model training, Oort prioritizes the use of those clients who have both data that offers the greatest utility in improving model accuracy and the capability to run training quickly. To enable FL developers to interpret their results in model testing, Oort enforces their requirements on the distribution of participant data while improving the duration of federated testing by cherry-picking clients. Our evaluation shows that, compared to existing participant selection mechanisms, Oort improves time-to-accuracy performance by 1.2X-14.1X and final model accuracy by 1.3%-9.8%, while efficiently enforcing developer-specified model testing criteria at the scale of millions of clients.\n",
      "<< year >> 2021\n",
      "<< href >> https://www.usenix.org/conference/osdi21/presentation/lai\n",
      "\n",
      "<< topic >> A survey on federated learning\n",
      "<< abstract >> AbstractFederated learning is a set-up in which multiple clients collaborate to solve machine learning problems, which is under the coordination of a central aggregator. This setting also allows the training data decentralized to ensure the data privacy of each device. Federated learning adheres to two major ideas: local computing and model transmission, which reduces some systematic privacy risks and costs brought by traditional centralized machine learning methods. The original data of the client is stored locally and cannot be exchanged or migrated. With the application of federated learning, each device uses local data for local training, then uploads the model to the server for aggregation, and finally the server sends the model update to the participants to achieve the learning goal. To provide a comprehensive survey and facilitate the potential research of this area, we systematically introduce the existing works of federated learning from five aspects: data partitioning, privacy mechanism, machine learning model, communication architecture and systems heterogeneity. Then, we sort out the current challenges and future research directions of federated learning. Finally, we summarize the characteristics of existing federated learning, and analyze the current practical application of federated learning.\n",
      "<< year >> 2021\n",
      "<< href >> https://www.sciencedirect.com/science/article/pii/S0950705121000381\n",
      "\n",
      "<< topic >> Communication-efficient federated learning\n",
      "<< abstract >> AbstractFederated learning (FL) enables edge devices, such as Internet of Things devices (e.g., sensors), servers, and institutions (e.g., hospitals), to collaboratively train a machine learning (ML) model without sharing their private data. FL requires devices to exchange their ML parameters iteratively, and thus the time it requires to jointly learn a reliable model depends not only on the number of training steps but also on the ML parameter transmission time per step. In practice, FL parameter transmissions are often carried out by a multitude of participating devices over resource-limited communication networks, for example, wireless networks with limited bandwidth and power. Therefore, the repeated FL parameter transmission from edge devices induces a notable delay, which can be larger than the ML model training time by orders of magnitude. Hence, communication delay constitutes a major bottleneck in FL. Here, a communication-efficient FL framework is proposed to jointly improve the FL convergence time and the training loss. In this framework, a probabilistic device selection scheme is designed such that the devices that can significantly improve the convergence speed and training loss have higher probabilities of being selected for ML model transmission. To further reduce the FL convergence time, a quantization method is proposed to reduce the volume of the model parameters exchanged among devices, and an efficient wireless resource allocation scheme is developed. Simulation results show that the proposed FL framework can improve the identification accuracy and convergence time by up to 3.6% and 87% compared to standard FL.\n",
      "<< year >> 2021\n",
      "<< href >> https://www.pnas.org/content/118/17/e2024789118.short\n",
      "\n",
      "<< topic >> System optimization of federated learning networks with a constrained latency\n",
      "<< abstract >> … Abstract—This paper investigates a wireless federated learning (FL) network with limited \n",
      "communication bandwidth, where multiple … and computational capability at the clients, we \n",
      "optimize the federated learning network by maximizing the number of active clients under the …\n",
      "<< year >> 2021\n",
      "<< href >> https://ieeexplore.ieee.org/abstract/document/9616432/\n",
      "\n",
      "<< topic >> Federated learning based on dynamic regularization\n",
      "<< abstract >> We propose a novel federated learning method for distributively trainingneural network models, where the server orchestrates cooperation between asubset of randomly chosen devices in each round. We view Federated Learningproblem primarily from a communication perspective and allow more device levelcomputations to save transmission costs. We point out a fundamental dilemma, inthat the minima of the local-device level empirical loss are inconsistent withthose of the global empirical loss. Different from recent prior works, thateither attempt inexact minimization or utilize devices for parallelizinggradient computation, we propose a dynamic regularizer for each device at eachround, so that in the limit the global and device solutions are aligned. Wedemonstrate both through empirical results on real and synthetic data as wellas analytical results that our scheme leads to efficient training, in bothconvex and non-convex settings, while being fully agnostic to deviceheterogeneity and robust to large number of devices, partial participation andunbalanced data.\n",
      "<< year >> 2021\n",
      "<< href >> https://arxiv.org/abs/2111.04263\n",
      "\n",
      "<< topic >> Multi-center federated learning\n",
      "<< abstract >> Federated learning (FL) can protect data privacy in distributed learningsince it merely collects local gradients from users without access to theirdata. However, FL is fragile in the presence of heterogeneity that is commonlyencountered in practical settings, e.g., non-IID data over different users.Existing FL approaches usually update a single global model to capture theshared knowledge of all users by aggregating their gradients, regardless of thediscrepancy between their data distributions. By comparison, a mixture ofmultiple global models could capture the heterogeneity across various users ifassigning the users to different global models (i.e., centers) in FL. To thisend, we propose a novel multi-center aggregation mechanism . It learns multipleglobal models from data, and simultaneously derives the optimal matchingbetween users and centers. We then formulate it as a bi-level optimizationproblem that can be efficiently solved by a stochastic expectation maximization(EM) algorithm. Experiments on multiple benchmark datasets of FL show that ourmethod outperforms several popular FL competitors. The source code are opensource on Github.\n",
      "<< year >> 2021\n",
      "<< href >> https://arxiv.org/abs/2108.08647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# path = './Data/scholar_analysis_FL_1401-1217.pbz2'\n",
    "path = newest_file_path\n",
    "res = decompressed_cpickle(path)\n",
    "page_res = res[list(res.keys())[0]]\n",
    "\n",
    "for contents in zip(*page_res.values()):\n",
    "#     print(contents)\n",
    "    for i in range(len(contents)):\n",
    "        print(\"<<\",list(page_res.keys())[i],\">>\", contents[i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
