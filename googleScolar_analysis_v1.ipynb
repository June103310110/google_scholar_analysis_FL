{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from utils import *\n",
    "from compress_utils import compressed_cpickle, decompressed_cpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(soup, href=True):\n",
    "    \n",
    "    h3_soup = soup.find_all('h3', class_=\"gs_rt\")\n",
    "    gs_rs_soup = soup.find_all('div', class_=\"gs_rs\")\n",
    "    gs_a_group = soup.find_all('div', class_='gs_a')\n",
    "    result = {'topic':[], 'abstract':[], \"year\":[]}\n",
    "    \n",
    "    valid_website = ['openaccess.thecvf.com', 'proceedings.neurips.cc', 'link.springer.com', 'dl.acm.org',\n",
    "         'arxiv.org', 'www.pnas.org', 'www.usenix.org', 'www.sciencedirect.com', 'www.nature.com',\n",
    "          'proceedings.mlr.press']\n",
    "    except_lis = ['ieeexplore.ieee.org']\n",
    "    valid_website = [i for i in valid_website if not i in except_lis]\n",
    "    \n",
    "    text_rule = lambda text:text.replace('.','_')\n",
    "    extract_func_dic = {}\n",
    "    for website in valid_website:\n",
    "        name_ = 'abs_from_'+text_rule(website)\n",
    "        extract_func_dic[website] = eval(name_)\n",
    "    \n",
    "\n",
    "    if href:\n",
    "        result['href'] = []\n",
    "        \n",
    "                \n",
    "    # get topic and abstract and href\n",
    "    for topic_res, abstract_res, attr_res in zip(h3_soup, gs_rs_soup, gs_a_group):\n",
    "        # get hef:\n",
    "        if href:\n",
    "            a = str(topic_res).split(' ')\n",
    "            href_ = [e[6:-1] for e in a if e.__contains__('href')][0]\n",
    "#             print(href_)\n",
    "            if href_:\n",
    "                result['href'].append(href_)\n",
    "            else:\n",
    "                result['href'].append('NaN')\n",
    "                print('else')\n",
    "   \n",
    "                \n",
    "#                 print(topic_res)\n",
    "        \n",
    "        # get year\n",
    "        year = [year for year in range(2015,2022) if attr_res.get_text().__contains__(str(year))]\n",
    "#         print(year[0])\n",
    "        if year:\n",
    "            result['year'].append(year[0])\n",
    "\n",
    "        else:\n",
    "            result['year'].append('NaN')\n",
    "            print('else, not found year, fill with nan')\n",
    "        \n",
    "        \n",
    "        # get topic\n",
    "        topic_text = topic_res.get_text()\n",
    "        b = '[HTML][HTML] '\n",
    "        topic_text = topic_text.replace(b, '')\n",
    "        result['topic'].append(topic_text)\n",
    "        \n",
    "        \n",
    "        # get abstract\n",
    "        website = href_.split('/')[2] \n",
    "        if [i for i in valid_website if i.__contains__(website)]:\n",
    "            abstract = extract_func_dic[website](href_)\n",
    "            result['abstract'].append(abstract)\n",
    "        else:\n",
    "            # google scholar default short abstract\n",
    "            if abstract_res.get_text():\n",
    "                result['abstract'].append(abstract_res.get_text())\n",
    "            else:\n",
    "                result['abstract'].append('NaN')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "# google_url = \"https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5\"\n",
    "# r = requests.get(google_url)\n",
    "# soup = BeautifulSoup(r.text, 'html.parser')\n",
    "# res = get_topic(soup)\n",
    "# # print(res)\n",
    "\n",
    "# for key in res:\n",
    "#     print(key, res[key])\n",
    "    \n",
    "# for contents in zip(res.values()):\n",
    "# #     print(contents)\n",
    "#     for i in range(len(contents)):\n",
    "#         print(list(res.keys())[i], contents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "page\n",
    "- [ok]. 0~30\n",
    "- [ok]. 30~60\n",
    "- [ok]. 60~90\n",
    "- [ok]. 90~120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=0\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=10\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=20\n",
      "else\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=30\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=40\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=50\n",
      "else\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=60\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=70\n",
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=80\n",
      "else\n"
     ]
    }
   ],
   "source": [
    "google_url = \"https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5\"\n",
    "end = 980\n",
    "dic = {}\n",
    "for i in range(end//10+1)[:30]:\n",
    "    a = google_url + f'&start={i*10}'\n",
    "    print(a)\n",
    "    r = requests.get(a)\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    res = get_topic(soup)\n",
    "#     print(res)\n",
    "#     lis.append()\n",
    "    dic['page_'+str(i)] = res\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['page_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pathlib\n",
    "\n",
    "path = './Data'\n",
    "pathlib.Path(f'{path}').mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d%m-%H%M\")\n",
    "# print(dt_string)\n",
    "compressed_cpickle(f'./Data/scholar_analysis_FL_{dt_string}', dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "pbz_paths = ['/'.join([path,i]) for i in os.listdir(path) if i.__contains__('.pbz2')]\n",
    "print(pbz_paths)\n",
    "newest_file_path = max(pbz_paths, key=os.path.getctime)\n",
    "newest_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newest_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './Data/scholar_analysis_FL_1401-1217.pbz2'\n",
    "path = newest_file_path\n",
    "res = decompressed_cpickle(path)\n",
    "page_res = res[list(res.keys())[0]]\n",
    "\n",
    "for contents in zip(*page_res.values()):\n",
    "#     print(contents)\n",
    "    for i in range(len(contents)):\n",
    "        print(\"<<\",list(page_res.keys())[i],\">>\", contents[i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
