{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from utils import *\n",
    "from compress_utils import compressed_cpickle, decompressed_cpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ./compress_utils.py\n",
    "# # data storage\n",
    "# import _pickle as cpkl\n",
    "# import bz2\n",
    "\n",
    "# # Pickle a file and then compress it into a file with extension \n",
    "# \"\"\"\n",
    "# usage: \n",
    "#     compressed_pickle('path/filename', data) \n",
    "# \"\"\"\n",
    "\n",
    "# def compressed_cpickle(path_title, data):\n",
    "#     with bz2.BZ2File(path_title+'.pbz2', \"w\") as f: \n",
    "#         cpkl.dump(data, f)\n",
    "\n",
    "        \n",
    "# # Load any compressed pickle file\n",
    "# \"\"\"\n",
    "# usage: \n",
    "#     data = decompressed_cpickle('example_cp.pbz2') \n",
    "# \"\"\"\n",
    "# def decompressed_cpickle(file):\n",
    "#     data = bz2.BZ2File(file, 'rb')\n",
    "#     data = cpkl.load(data)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_website = ['openaccess.thecvf.com', 'proceedings.neurips.cc', 'link.springer.com', 'dl.acm.org',\n",
    "#          'arxiv.org', 'www.pnas.org', 'www.usenix.org', 'www.sciencedirect.com', 'www.nature.com',\n",
    "#          'ieeexplore.ieee.org', 'proceedings.mlr.press']\n",
    "\n",
    "# # website = 'openaccess.thecvf.com'\n",
    "# text_rule = lambda text:text.replace('.','_')\n",
    "# extract_func_dic = {}\n",
    "# for website in valid_website:\n",
    "#     name_ = 'abs_from_'+text_rule(website)\n",
    "#     extract_func_dic[name_] = eval(name_)\n",
    "# extract_func_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(soup, href=True):\n",
    "    \n",
    "    h3_soup = soup.find_all('h3', class_=\"gs_rt\")\n",
    "    gs_rs_soup = soup.find_all('div', class_=\"gs_rs\")\n",
    "    gs_a_group = soup.find_all('div', class_='gs_a')\n",
    "    result = {'topic':[], \"year\":[]}\n",
    "    \n",
    "    valid_website = ['openaccess.thecvf.com', 'proceedings.neurips.cc', 'link.springer.com', 'dl.acm.org',\n",
    "         'arxiv.org', 'www.pnas.org', 'www.usenix.org', 'www.sciencedirect.com', 'www.nature.com',\n",
    "         'ieeexplore.ieee.org', 'proceedings.mlr.press']\n",
    "    except_lis = ['ieeexplore.ieee.org']\n",
    "    valid_website = [i for i in valid_website if not i in except_lis]\n",
    "    \n",
    "    text_rule = lambda text:text.replace('.','_')\n",
    "    extract_func_dic = {}\n",
    "    for website in valid_website:\n",
    "        name_ = 'abs_from_'+text_rule(website)\n",
    "        extract_func_dic[website] = eval(name_)\n",
    "    \n",
    "\n",
    "    if href:\n",
    "        result['href'] = []\n",
    "        \n",
    "                \n",
    "    # get topic and abstract and href\n",
    "    for topic, abstract, attr in zip(h3_soup, gs_rs_soup, gs_a_group):\n",
    "        # get hef:\n",
    "        if href:\n",
    "            a = str(topic).split(' ')\n",
    "            href_ = [e[6:-1] for e in a if e.__contains__('href')][0]\n",
    "    #             print(href_)\n",
    "            result['href'].append(href_)\n",
    "        \n",
    "        # get year\n",
    "        for i in [str(year) for year in range(2015,2022)]:\n",
    "            if attr.get_text().__contains__(i):\n",
    "                result['year'].append(i)\n",
    "#                 print(i)\n",
    "        \n",
    "        # get topic\n",
    "        topic_text = topic.get_text()\n",
    "        b = '[HTML][HTML] '\n",
    "        topic_text = topic_text.replace(b, '')\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # get abstract\n",
    "\n",
    "        text_rule = lambda text:text.replace('.','_')\n",
    "        \n",
    "        for i in valid_website:\n",
    "            if href_.__contains__(i):\n",
    "                website = href_.split('/')[2]\n",
    "                abstract = extract_func_dic[website](href_)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        result['topic'].append((topic_text, 123))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def abs_from_ieeexplore_ieee_org(url): \n",
    "#     website = 'ieeexplore.ieee.org'\n",
    "#     if url.__contains__(website):\n",
    "#         pass\n",
    "#     else:\n",
    "#         raise KeyError(f'This function is only for web {website}')\n",
    "#     r = requests.get(url)\n",
    "#     abstract = r.text.split('meta property=\"og:description\"')#[1].split('content=\"')[1].split('\"')[0]\n",
    "#     return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://ieeexplore.ieee.org/abstract/document/9084352'\n",
    "# # r\n",
    "# # soup = BeautifulSoup(r.text, 'html.parser')\n",
    "# # res = get_topic(soup)\n",
    "# a = abs_from_ieeexplore_ieee_org(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=0\n",
      "{'topic': [('Ditto: Fair and robust federated learning through personalization', 123), ('Federated learning for internet of things: Recent advances, taxonomy, and open challenges', 123), ('Federated learning for predicting clinical outcomes in patients with COVID-19', 123), ('A survey on security and privacy of federated learning', 123), ('Oort: Efficient federated learning via guided participant selection', 123), ('A survey on federated learning', 123), ('Communication-efficient federated learning', 123), ('System optimization of federated learning networks with a constrained latency', 123), ('Federated learning based on dynamic regularization', 123), ('Multi-center federated learning', 123)], 'year': ['2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021'], 'href': ['https://proceedings.mlr.press/v139/li21h.html', 'https://ieeexplore.ieee.org/abstract/document/9460016/', 'https://www.nature.com/articles/s41591-021-01506-3', 'https://www.sciencedirect.com/science/article/pii/S0167739X20329848', 'https://www.usenix.org/conference/osdi21/presentation/lai', 'https://www.sciencedirect.com/science/article/pii/S0950705121000381', 'https://www.pnas.org/content/118/17/e2024789118.short', 'https://ieeexplore.ieee.org/abstract/document/9616432/', 'https://arxiv.org/abs/2111.04263', 'https://arxiv.org/abs/2108.08647']}\n"
     ]
    }
   ],
   "source": [
    "google_url = \"https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5\"\n",
    "end = 980\n",
    "dic = {}\n",
    "for i in range(end//10+1)[:1]:\n",
    "    a = google_url + f'&start={i*10}'\n",
    "    print(a)\n",
    "    r = requests.get(a)\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    res = get_topic(soup)\n",
    "    print(res)\n",
    "#     lis.append()\n",
    "    dic['page_'+str(i*10)] = res\n",
    "    time.sleep(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pathlib\n",
    "\n",
    "path = './Data'\n",
    "pathlib.Path(f'{path}').mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d%m-%H%M\")\n",
    "# print(dt_string)\n",
    "compressed_cpickle(f'./Data/scholar_analysis_FL_{dt_string}', dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Data/scholar_analysis_FL_1401-1107.pbz2', './Data/scholar_analysis_FL_1401-1109.pbz2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./Data/scholar_analysis_FL_1401-1109.pbz2'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "pbz_paths = ['/'.join([path,i]) for i in os.listdir(path) if i.__contains__('.pbz2')]\n",
    "print(pbz_paths)\n",
    "newest_file_path = max(pbz_paths, key=os.path.getctime)\n",
    "newest_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_0': {'topic': [('Ditto: Fair and robust federated learning through personalization',\n",
       "    123),\n",
       "   ('Federated learning for internet of things: Recent advances, taxonomy, and open challenges',\n",
       "    123),\n",
       "   ('Federated learning for predicting clinical outcomes in patients with COVID-19',\n",
       "    123),\n",
       "   ('A survey on security and privacy of federated learning', 123),\n",
       "   ('Oort: Efficient federated learning via guided participant selection',\n",
       "    123),\n",
       "   ('A survey on federated learning', 123),\n",
       "   ('Communication-efficient federated learning', 123),\n",
       "   ('System optimization of federated learning networks with a constrained latency',\n",
       "    123),\n",
       "   ('Federated learning based on dynamic regularization', 123),\n",
       "   ('Multi-center federated learning', 123)],\n",
       "  'year': ['2021',\n",
       "   '2021',\n",
       "   '2021',\n",
       "   '2021',\n",
       "   '2021',\n",
       "   '2021',\n",
       "   '2021',\n",
       "   '2021',\n",
       "   '2021',\n",
       "   '2021'],\n",
       "  'href': ['https://proceedings.mlr.press/v139/li21h.html',\n",
       "   'https://ieeexplore.ieee.org/abstract/document/9460016/',\n",
       "   'https://www.nature.com/articles/s41591-021-01506-3',\n",
       "   'https://www.sciencedirect.com/science/article/pii/S0167739X20329848',\n",
       "   'https://www.usenix.org/conference/osdi21/presentation/lai',\n",
       "   'https://www.sciencedirect.com/science/article/pii/S0950705121000381',\n",
       "   'https://www.pnas.org/content/118/17/e2024789118.short',\n",
       "   'https://ieeexplore.ieee.org/abstract/document/9616432/',\n",
       "   'https://arxiv.org/abs/2111.04263',\n",
       "   'https://arxiv.org/abs/2108.08647']}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decompressed_cpickle(newest_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://scholar.google.com.tw/scholar?as_ylo=2021&q=federated+learning&hl=zh-TW&as_sdt=0,5&start=0'\n",
    "\n",
    "# # headers = {'UserAgent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "# r = requests.get(url)\n",
    "# r.text\n",
    "# # soup = BeautifulSoup(r.text, 'html.parser')\n",
    "# # soup"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# if __name__ == '__main__':\n",
    "#     r = requests.get(google_url)\n",
    "#     soup = BeautifulSoup(r.text, 'html.parser')\n",
    "#     res = get_topic(soup)\n",
    "#     print(res)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text_rule = lambda text:text.replace('.','_')\n",
    "website = 'www.sciencedirect.com'\n",
    "text_rule(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AbstractFederated learning (FL) is a new breed of Artificial Intelligence (AI) that builds upon decentralized data and training that brings learning to the edge or directly on-device. FL is a new research area often referred to as a new dawn in AI, is in its infancy, and has not yet gained much trust in the community, mainly because of its (unknown) security and privacy implications. To advance the state of the research in this area and to realize extensive utilization of the FL approach and its mass adoption, its security and privacy concerns must be first identified, evaluated, and documented. FL is preferred in use-cases where security and privacy are the key concerns and having a clear view and understanding of risk factors enable an implementer/adopter of FL to successfully build a secure environment and gives researchers a clear vision on possible research areas. This paper aims to provide a comprehensive study concerning FLâ€™s security and privacy aspects that can help bridge the gap between the current state of federated AI and a future in which mass adoption is possible. We present an illustrative description of approaches and various implementation styles with an examination of the current challenges in FL and establish a detailed review of security and privacy concerns that need to be considered in a thorough and clear context. Findings from our study suggest that overall there are fewer privacy-specific threats associated with FL compared to security threats. The most specific security threats currently are communication bottlenecks, poisoning, and backdoor attacks while inference-based attacks are the most critical to the privacy of FL. We conclude the paper with much needed future research directions to make FL adaptable in realistic scenarios.'"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r = requests.get(url)\n",
    "\n",
    "def abs_from_www_sciencedirect_com(url): \n",
    "    \n",
    "    website = 'www.sciencedirect.com'\n",
    "    if url.__contains__(website):\n",
    "        pass\n",
    "    else:\n",
    "        raise KeyError(f'This function is only for web {website}')\n",
    "    \n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    r.text\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    res = soup.find_all('div', class_ = 'abstract author')[0]\n",
    "    \n",
    "\n",
    "    return res.get_text()\n",
    "abs_from_www_sciencedirect_com(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.get(url)\n",
    "\n",
    "def abs_from_academic_oup_com(url): \n",
    "    \n",
    "    website = 'academic.oup.com'\n",
    "    if url.__contains__(website):\n",
    "        pass\n",
    "    else:\n",
    "        raise KeyError(f'This function is only for web {website}')\n",
    "        \n",
    "        \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    res = soup.find_all('section', class_ = 'abstract')\n",
    "    # res.find('div', class_='sec')\n",
    "    abstract = ''\n",
    "    for sec in res:\n",
    "    #     sec.find('div', class_='sec')\n",
    "        for sub_sec in sec:\n",
    "            a = sub_sec.contents[0].get_text()\n",
    "            b = sub_sec.contents[1].get_text()\n",
    "            abstract += a+' '\n",
    "            abstract += b\n",
    "    return abstract\n",
    "# abs_from_academic_oup_com(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_from_openaccess_thecvf_com(url): \n",
    "\n",
    "    website = 'openaccess.thecvf.com'\n",
    "    if url.__contains__(website):\n",
    "        pass\n",
    "    else:\n",
    "        raise KeyError(f'This function is only for web {website}')\n",
    "\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    res = soup.find_all('div', id='abstract')\n",
    "    abstract = \"\"\n",
    "    for i in res:\n",
    "#         print(i)\n",
    "        abstract+= i.get_text()\n",
    "    return abstract.replace('\\n', '').strip() #.split('Abstract: ')[1]\n",
    "\n",
    "# abs_from_openaccess_thecvf_com(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_from_proceedings_neurips_cc(url): \n",
    "\n",
    "    website = 'proceedings.neurips.cc'\n",
    "    if url.__contains__(website):\n",
    "        pass\n",
    "    else:\n",
    "        raise KeyError(f'This function is only for web {website}')\n",
    "\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    res= soup.find_all('p')\n",
    "\n",
    "    max_length = max([len(i.get_text()) for i in res])\n",
    "    for i in res:\n",
    "        if len(i.get_text()) == max_length:\n",
    "#             print(i.get_text())  \n",
    "            return i.get_text()\n",
    "\n",
    "# abs_from_proceedings_neurips_cc(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_from_link_springer_com(url): \n",
    "\n",
    "    website = 'link.springer.com'\n",
    "    if url.__contains__(website):\n",
    "        pass\n",
    "    else:\n",
    "        raise KeyError(f'This function is only for web {website}')\n",
    "\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    res = soup.find_all('div',  class_='c-article-section__content', id='Abs1-content')\n",
    "    abstract = \"\"\n",
    "    for i in res:\n",
    "#         print(i)\n",
    "        abstract+= i.get_text()\n",
    "    return abstract #.split('Abstract: ')[1].replace('\\n', '').strip()\n",
    "\n",
    "# abs_from_link_springer_com(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_from_dl_acm_org(url): \n",
    "\n",
    "    website = 'dl.acm.org'\n",
    "    if url.__contains__(website):\n",
    "        pass\n",
    "    else:\n",
    "        raise KeyError(f'This function is only for web {website}')\n",
    "\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    res = soup.find_all('div',  class_=\"abstractSection abstractInFull\")\n",
    "    abstract = \"\"\n",
    "    for i in res:\n",
    "#         print(i)\n",
    "        abstract+= i.get_text()\n",
    "    return abstract #.split('Abstract: ')[1].replace('\\n', '').strip()\n",
    "\n",
    "# abs_from_dl_acm_org(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_from_arxiv_org(url): \n",
    "\n",
    "    website = 'arxiv.org'\n",
    "    if url.__contains__(website):\n",
    "        pass\n",
    "    else:\n",
    "        raise KeyError(f'This function is only for web {website}')\n",
    "\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    res = soup.find_all('blockquote',  class_=\"abstract mathjax\")\n",
    "    abstract = \"\"\n",
    "    for i in res:\n",
    "#         print(i)\n",
    "        abstract+= i.get_text()\n",
    "    return abstract.split('Abstract: ')[1].replace('\\n', '').strip()\n",
    "\n",
    "# abs_from_arxiv_org(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_from_www_pnas_org(url): \n",
    "\n",
    "    website = 'www.pnas.org'\n",
    "    if url.__contains__(website):\n",
    "        pass\n",
    "    else:\n",
    "        raise KeyError(f'This function is only for web {website}')\n",
    "\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    res = soup.find_all('div',  class_=\"section abstract\")\n",
    "    abstract = \"\"\n",
    "    for i in res:\n",
    "    #         print(i.get_text())\n",
    "        abstract+= i.get_text()\n",
    "    return abstract\n",
    "\n",
    "# abs_from_www_pnas_org(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_from_www_usenix_org(url): \n",
    "    website = 'www.usenix.org'\n",
    "    if url.__contains__(website):\n",
    "        pass\n",
    "    else:\n",
    "        raise KeyError(f'This function is only for web {website}')\n",
    "        \n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    res = soup.find_all('div',  class_=\"field field-name-field-paper-description field-type-text-long field-label-above\")\n",
    "    abstract = \"\"\n",
    "    for i in res:\n",
    "#         print(i.get_text())\n",
    "        abstract+= i.get_text()\n",
    "    return abstract\n",
    "#     soup_ = BeautifulSoup(i, 'html.parser')\n",
    "\n",
    "# abs_from_www_usenix_org(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def abs_from_ieeexplore_ieee_org(url): \n",
    "# print(url)\n",
    "def abs_from_www_nature_com(url): \n",
    "    website = 'www.nature.com'\n",
    "    if url.__contains__(website):\n",
    "        pass\n",
    "    else:\n",
    "        raise KeyError(f'This function is only for web {website}')\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    res = soup.find_all('div',  class_=\"c-article-section__content\")\n",
    "    return [ i.get_text().strip() for i in res][0]\n",
    "\n",
    "# abs_from_www_nature_com(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if url.__contains__(website):\n",
    "#     print(url)\n",
    "#     pass\n",
    "def abs_from_ieeexplore_ieee_org(url): \n",
    "    website = 'ieeexplore.ieee.org'\n",
    "    if url.__contains__(website):\n",
    "        pass\n",
    "    else:\n",
    "        raise KeyError(f'This function is only for web {website}')\n",
    "    r = requests.get(url)\n",
    "    abstract = r.text.split('meta property=\"og:description\"')[1].split('content=\"')[1].split('\"')[0]\n",
    "    return abstract\n",
    "# abs_from_ieeexplore_ieee_org(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_from_proceedings_mlr_press(url):\n",
    "    website = 'proceedings.mlr.press'\n",
    "    if url.__contains__(website):\n",
    "        pass\n",
    "    else:\n",
    "        raise KeyError(f'This function is only for web {website}')\n",
    "    r = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    res = soup.find_all('div',  class_=\"abstract\")\n",
    "    return [ i.get_text().strip() for i in res][0]\n",
    "\n",
    "\n",
    "# abs_from_proceedings_mlr_press(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_des(dic_gs_rs):\n",
    "#     des = []\n",
    "#     for i in dic_gs_rs:\n",
    "#         a = i.get_text()\n",
    "#         des.append(a)\n",
    "#     return des\n",
    "# if __name__ == '__main__':\n",
    "#     res = get_des(soup.find_all('div',  class_=\"gs_rs\"))\n",
    "#     print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
